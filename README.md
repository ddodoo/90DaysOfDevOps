# Ethereum Pod Scheduling Issue - Diagnosis and Fix

## Problem Summary

The lighthouse pod is failing to schedule with the following error:
```
0/3 nodes are available: pod has unbound immediate PersistentVolumeClaims. 
preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
```

## Root Cause Analysis

The error indicates three main issues:

### 1. **Unbound PersistentVolumeClaims (Primary Issue)**
- The PVCs are using `storageClassName: standard` which may not exist in your cluster
- The storage class might use `Immediate` binding mode, requiring nodes in the same zone as storage
- Volume node affinity conflicts prevent the PVC from binding to available nodes

### 2. **Insufficient Memory Resources**
- Some nodes have insufficient memory for the lighthouse pod (requested 2Gi)
- Node capacity is limited, especially on smaller instance types

### 3. **Node Affinity Conflicts**
- Storage volumes have zone affinity requirements that conflict with available nodes
- Persistent disks in GKE must be in the same zone as the nodes mounting them

## Solutions Provided

### üîß Quick Fix Script (`fix-ethereum-pods.sh`)
An automated script that:
- Detects the correct storage class for your cluster
- Reduces resource requirements for better scheduling
- Cleans up problematic resources and recreates them
- Monitors the deployment process

**Usage:**
```bash
./fix-ethereum-pods.sh
```

### üìã Fixed YAML Configuration (`ethereum-fixed.yaml`)
A comprehensive fix with:
- Updated storage class (`standard-rwo` instead of `standard`)
- Reduced resource requirements for lighthouse pod
- Added node affinity preferences
- Improved tolerations for better scheduling
- Added missing Service definition for geth

**Usage:**
```bash
kubectl apply -f ethereum-fixed.yaml
```

### üîç Troubleshooting Script (`troubleshoot-ethereum.sh`)
A diagnostic tool that:
- Checks kubectl configuration
- Analyzes storage classes and PVC status
- Examines node resources and pod scheduling
- Provides specific recommendations for your cluster

**Usage:**
```bash
./troubleshoot-ethereum.sh
```

## Key Changes Made

### Storage Configuration
```yaml
# Before (problematic)
storageClassName: standard

# After (fixed)
storageClassName: standard-rwo  # or auto-detected suitable class
```

### Resource Requirements
```yaml
# Lighthouse - Before
resources:
  requests:
    memory: "2Gi"
    cpu: "500m"
  limits:
    memory: "4Gi"
    cpu: "1000m"

# Lighthouse - After
resources:
  requests:
    memory: "1Gi"      # Reduced from 2Gi
    cpu: "250m"        # Reduced from 500m
  limits:
    memory: "2Gi"      # Reduced from 4Gi
    cpu: "500m"        # Reduced from 1000m
```

### Added Node Affinity
```yaml
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: node.kubernetes.io/instance-type
          operator: NotIn
          values: ["f1-micro", "g1-small", "e2-micro", "e2-small"]
```

## Manual Fix Steps

If you prefer to fix manually:

1. **Check available storage classes:**
   ```bash
   kubectl get storageclass
   ```

2. **Delete problematic PVCs:**
   ```bash
   kubectl delete pvc lighthouse-data-pvc geth-data-pvc -n ethereum
   ```

3. **Update the storage class in your YAML:**
   - Replace `storageClassName: standard` with a valid class like `standard-rwo`

4. **Reduce resource requirements:**
   - Lower lighthouse memory request to 1Gi
   - Reduce CPU requests if needed

5. **Apply the updated configuration:**
   ```bash
   kubectl apply -f your-fixed-file.yaml
   ```

## Common Storage Classes by Platform

- **GKE**: `standard-rwo`, `ssd`, `fast-ssd`
- **EKS**: `gp2`, `gp3`, `io1`, `io2`
- **AKS**: `default`, `managed-premium`
- **Minikube**: `standard`

## Monitoring and Verification

After applying the fix:

```bash
# Check PVC status
kubectl get pvc -n ethereum

# Monitor pod status
kubectl get pods -n ethereum -w

# Check pod details if still pending
kubectl describe pod <pod-name> -n ethereum

# View logs once running
kubectl logs -f deployment/lighthouse -n ethereum
```

## Prevention Tips

1. **Always check storage classes before deployment:**
   ```bash
   kubectl get storageclass
   ```

2. **Use resource requests that fit your node capacity:**
   ```bash
   kubectl describe nodes
   ```

3. **Consider using regional persistent disks for multi-zone clusters**

4. **Enable cluster autoscaler for dynamic node scaling**

5. **Test with smaller resource requirements first**

## Additional Resources

- [Kubernetes Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/)
- [Pod Scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
- [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
- [GKE Storage Guide](https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes)
